import sys
import os
import torch
from torch.utils.data import DataLoader, random_split
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger

# --- å…³é”®ï¼šå°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°æœç´¢è·¯å¾„ï¼Œç¡®ä¿èƒ½ import src ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

from src.datasets.ct_dataset import CTSuperResDataset
from src.core.reconstruction import SRTrainer


def main():
    # ================= é…ç½®åŒºåŸŸ =================
    DATA_DIR = os.path.join(project_root, "data", "raw")  # æ•°æ®è·¯å¾„
    BATCH_SIZE = 8  # å¦‚æœæ˜¾å­˜ä¸å¤Ÿ(å¦‚<4G)ï¼Œæ”¹ä¸º 4 æˆ– 2
    NUM_WORKERS = 0  # Windowsä¸‹å»ºè®®å…ˆè®¾ä¸º0ï¼Œè°ƒè¯•æ²¡é—®é¢˜åå†æ”¹ä¸º 2 æˆ– 4
    MAX_EPOCHS = 100  # è®­ç»ƒè½®æ•°
    LR = 1e-4  # å­¦ä¹ ç‡
    SCALE_FACTOR = 4  # è¶…åˆ†å€æ•°
    CROP_SIZE = 128  # è®­ç»ƒåˆ‡ç‰‡å¤§å° (HR)
    # ===========================================

    # 1. è®¾ç½®éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
    pl.seed_everything(42)

    # 2. å‡†å¤‡æ•°æ®
    print(f"æ­£åœ¨åŠ è½½æ•°æ®: {DATA_DIR} ...")
    full_dataset = CTSuperResDataset(
        data_dir=DATA_DIR,
        phase='train',
        crop_size=CROP_SIZE,
        scale_factor=SCALE_FACTOR
    )

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (9:1)
    train_size = int(0.9 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])

    print(f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ -> è®­ç»ƒé›†: {len(train_dataset)} å¼ , éªŒè¯é›†: {len(val_dataset)} å¼ ")

    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=NUM_WORKERS,
        pin_memory=True if torch.cuda.is_available() else False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=NUM_WORKERS
    )

    # 3. åˆå§‹åŒ–æ¨¡å‹
    model = SRTrainer(lr=LR)

    # 4. é…ç½®å›è°ƒå‡½æ•° (ä¿å­˜æ¨¡å‹å’Œç›‘æ§å­¦ä¹ ç‡)
    checkpoint_callback = ModelCheckpoint(
        dirpath=os.path.join(project_root, "checkpoints", "sr_model"),
        filename='srgan-{epoch:02d}-{g_loss:.4f}',
        save_top_k=3,  # ä¿å­˜æœ€å¥½çš„3ä¸ªæ¨¡å‹
        monitor='g_loss',  # æ ¹æ®ç”Ÿæˆå™¨æŸå¤±åˆ¤æ–­å¥½å
        mode='min'
    )

    lr_monitor = LearningRateMonitor(logging_interval='step')

    # 5. é…ç½®è®­ç»ƒå™¨
    logger = TensorBoardLogger(os.path.join(project_root, "logs"), name="sr_experiment")

    trainer = Trainer(
        max_epochs=MAX_EPOCHS,
        accelerator="gpu" if torch.cuda.is_available() else "cpu",
        devices=1,
        logger=logger,
        callbacks=[checkpoint_callback, lr_monitor],
        log_every_n_steps=5,  # æ¯5æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
        # limit_train_batches=0.1 # è°ƒè¯•æ—¶å–æ¶ˆæ³¨é‡Šï¼Œåªè·‘10%çš„æ•°æ®å¿«é€Ÿæµ‹è¯•æµç¨‹
    )

    # 6. å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ SRGAN æ¨¡å‹...")
    print(f"æ—¥å¿—å°†ä¿å­˜åœ¨: {logger.log_dir}")
    print("å¯ä»¥ä½¿ç”¨ tensorboard --logdir logs æŸ¥çœ‹è®­ç»ƒæ›²çº¿")

    trainer.fit(model, train_loader, val_loader)


# ç”±äº windows ä¸‹å¤šè¿›ç¨‹çš„é™åˆ¶ï¼Œå¿…é¡»åŠ è¿™ä¸ªä¿æŠ¤
if __name__ == "__main__":
    from pytorch_lightning import Trainer  # å»¶è¿Ÿå¯¼å…¥é¿å…æŸäº›å¾ªç¯å¼•ç”¨

    main()