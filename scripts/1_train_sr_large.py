import sys
import os
import torch
import torchvision
import numpy as np
import pydicom
import glob
from datetime import datetime  # <--- æ–°å¢žæ—¶é—´æ¨¡å—
from torch.utils.data import DataLoader, random_split, Dataset
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, Callback
from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger
from pytorch_lightning import Trainer
import torchvision.transforms.functional as F
from torchvision import transforms

# --- è·¯å¾„è®¾ç½® ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

from src.core.reconstruction import SRTrainer


# ==========================================
# Dataset (ä¿æŒä¸å˜)
# ==========================================
class CTLargeDataset(Dataset):
    def __init__(self, data_dir, phase='train', crop_size=128, scale_factor=4):
        super().__init__()
        self.phase = phase
        self.crop_size = crop_size
        self.scale_factor = scale_factor
        print(f"[{phase}] æ­£åœ¨æ‰«æè·¯å¾„: {data_dir}")
        self.dcm_files = sorted(glob.glob(os.path.join(data_dir, "**/*.dcm"), recursive=True))
        if len(self.dcm_files) == 0:
            raise ValueError(f"é”™è¯¯: åœ¨è·¯å¾„ {data_dir} ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .dcm æ–‡ä»¶ï¼")
        print(f"[{phase}] æ‰«æå®Œæˆï¼å…±å‘çŽ° {len(self.dcm_files)} å¼  DICOM åˆ‡ç‰‡ã€‚")
        print(f"[{phase}] æ¨¡å¼: ç¡¬ç›˜ç›´è¯» (Disk I/O) - å†…å­˜å ç”¨å°†ä¸¥æ ¼é™åˆ¶ã€‚")

    def __len__(self):
        return len(self.dcm_files)

    def __getitem__(self, index):
        dcm_path = self.dcm_files[index]
        try:
            ds = pydicom.dcmread(dcm_path)
            try:
                ds.decompress()
            except:
                pass

            if hasattr(ds, 'pixel_array'):
                slope = getattr(ds, 'RescaleSlope', 1.0)
                intercept = getattr(ds, 'RescaleIntercept', 0.0)
                img_hu = ds.pixel_array.astype(np.float32) * slope + intercept
                img_norm = np.clip(img_hu, -1000, 2000)
                img_norm = (img_norm + 1000) / 3000.0
                hr_tensor = torch.from_numpy(img_norm).unsqueeze(0).float()
            else:
                return self.__getitem__(np.random.randint(0, len(self.dcm_files)))

            if hr_tensor.shape[1] < self.crop_size or hr_tensor.shape[2] < self.crop_size:
                return self.__getitem__(np.random.randint(0, len(self.dcm_files)))

            if self.phase == 'train':
                i, j, h, w = transforms.RandomCrop.get_params(hr_tensor, output_size=(self.crop_size, self.crop_size))
                hr_tensor = F.crop(hr_tensor, i, j, h, w)
                if torch.rand(1) < 0.5: hr_tensor = F.hflip(hr_tensor)
                if torch.rand(1) < 0.5: hr_tensor = F.vflip(hr_tensor)

            lr_h, lr_w = hr_tensor.shape[1] // self.scale_factor, hr_tensor.shape[2] // self.scale_factor
            lr_tensor = torch.nn.functional.interpolate(hr_tensor.unsqueeze(0), size=(lr_h, lr_w), mode='bicubic',
                                                        align_corners=False).squeeze(0)
            return {"lr": lr_tensor, "hr": hr_tensor}
        except Exception:
            return self.__getitem__(np.random.randint(0, len(self.dcm_files)))


# ==========================================
# ImageLogger (ä¿æŒä¸å˜)
# ==========================================
class ImageLogger(Callback):
    def __init__(self, num_samples=4):
        super().__init__()
        self.num_samples = num_samples

    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):
        if batch_idx == 0:
            try:
                lr = batch["lr"][:self.num_samples]
                hr = batch["hr"][:self.num_samples]
                with torch.no_grad():
                    sr = pl_module(lr)
                lr_upscaled = torch.nn.functional.interpolate(lr, size=hr.shape[2:], mode='nearest')
                grid_lr = torchvision.utils.make_grid(lr_upscaled, nrow=4, normalize=True)
                grid_sr = torchvision.utils.make_grid(sr, nrow=4, normalize=True)
                grid_hr = torchvision.utils.make_grid(hr, nrow=4, normalize=True)
                # æ³¨æ„ï¼šå½“ logger æ˜¯åˆ—è¡¨æ—¶ï¼Œéœ€è¦éåŽ†æˆ–è€…æŒ‡å®šä¸€ä¸ª
                for logger in trainer.loggers:
                    if isinstance(logger, TensorBoardLogger):
                        logger.experiment.add_image('1_Input_LowRes', grid_lr, trainer.global_step)
                        logger.experiment.add_image('2_Output_SuperRes', grid_sr, trainer.global_step)
                        logger.experiment.add_image('3_GroundTruth_HighRes', grid_hr, trainer.global_step)
            except Exception:
                pass


def get_latest_checkpoint(checkpoints_root):
    """
    è¾…åŠ©å‡½æ•°ï¼šæ‰«ææ‰€æœ‰æ—¶é—´æˆ³æ–‡ä»¶å¤¹ï¼Œæ‰¾åˆ°æœ€è¿‘ä¸€æ¬¡ä¿®æ”¹çš„ last.ckpt
    """
    if not os.path.exists(checkpoints_root):
        return None

    # èŽ·å–æ‰€æœ‰å­æ–‡ä»¶å¤¹ (å³æ‰€æœ‰çš„æ—¶é—´æˆ³æ–‡ä»¶å¤¹)
    all_runs = [os.path.join(checkpoints_root, d) for d in os.listdir(checkpoints_root) if
                os.path.isdir(os.path.join(checkpoints_root, d))]
    if not all_runs:
        return None

    # æŒ‰ä¿®æ”¹æ—¶é—´æŽ’åºï¼Œæœ€è¿‘çš„åœ¨æœ€åŽ
    all_runs.sort(key=os.path.getmtime)

    # ä»Žæœ€æ–°çš„æ–‡ä»¶å¤¹å¼€å§‹å¾€å‰æ‰¾ last.ckpt
    for run_dir in reversed(all_runs):
        ckpt_path = os.path.join(run_dir, "last.ckpt")
        if os.path.exists(ckpt_path):
            return ckpt_path

    return None


def main():
    # ================= é…ç½®åŒºåŸŸ =================
    CUSTOM_DATA_PATH = r"D:\database\CIPæ•°æ®é›†\CTå½±åƒ dcmæ ¼å¼"
    # CUSTOM_DATA_PATH = r"D:\project\SpineTwin-PathPlanner\data"

    BATCH_SIZE = 128
    NUM_WORKERS = 6
    MAX_EPOCHS = 20
    LR = 1e-4
    CROP_SIZE = 128
    SCALE_FACTOR = 4
    # ===========================================

    pl.seed_everything(42)

    # --- 1. ç”Ÿæˆå½“å‰æ—¶é—´æˆ³ (Run ID) ---
    # æ ¼å¼: 2026-01-22_17-05
    run_id = datetime.now().strftime("%Y-%m-%d_%H-%M")

    # å®šä¹‰æœ¬æ¬¡è¿è¡Œçš„ä¸“å±žç›®å½•
    current_ckpt_dir = os.path.join(project_root, "checkpoints", run_id)
    current_log_dir = os.path.join(project_root, "logs", run_id)

    if not os.path.exists(CUSTOM_DATA_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è·¯å¾„: {CUSTOM_DATA_PATH}")
        return

    print(f"ðŸš€ åˆå§‹åŒ–å¤§æ•°æ®é›†åŠ è½½å™¨...")
    full_dataset = CTLargeDataset(CUSTOM_DATA_PATH, phase='train', crop_size=CROP_SIZE, scale_factor=SCALE_FACTOR)

    val_count = 2000
    if len(full_dataset) < val_count: val_count = int(0.1 * len(full_dataset))
    train_size = len(full_dataset) - val_count
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_count])

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,
                              pin_memory=True, persistent_workers=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS,
                            persistent_workers=True)

    model = SRTrainer(lr=LR)

    # --- 2. é…ç½®å›žè°ƒä¸Žæ—¥å¿— (ä½¿ç”¨æ—¶é—´æˆ³è·¯å¾„) ---
    checkpoint_callback = ModelCheckpoint(
        dirpath=current_ckpt_dir,  # æ¨¡åž‹å­˜åˆ° checkpoints/æ—¶é—´/ ä¸‹
        filename='srgan-{epoch:02d}-{val_psnr:.2f}',
        save_top_k=-1,  # ä¿å­˜æ‰€æœ‰
        every_n_epochs=1,
        monitor='val_psnr',
        mode='max',
        save_last=True
    )

    # æ—¥å¿—å­˜åˆ° logs/æ—¶é—´/ ä¸‹
    # name="" å’Œ version="" æ˜¯ä¸ºäº†ä¸è®©å®ƒå†è‡ªåŠ¨åˆ›å»º version_0 å­æ–‡ä»¶å¤¹ï¼Œç›´æŽ¥ç”¨æˆ‘ä»¬æŒ‡å®šçš„æ—¶é—´æ–‡ä»¶å¤¹
    tb_logger = TensorBoardLogger(save_dir=current_log_dir, name="", version="")
    csv_logger = CSVLogger(save_dir=current_log_dir, name="", version="")

    img_logger = ImageLogger(num_samples=4)

    torch.backends.cudnn.benchmark = True
    torch.set_float32_matmul_precision('medium')

    trainer = Trainer(
        max_epochs=MAX_EPOCHS,
        accelerator="gpu",
        devices=1,
        logger=[tb_logger, csv_logger],
        callbacks=[checkpoint_callback, LearningRateMonitor(logging_interval='step'), img_logger],
        log_every_n_steps=50,
        check_val_every_n_epoch=1,
        precision="16-mixed",
    )

    # --- 3. æ™ºèƒ½ç»­è®­é€»è¾‘ ---
    # æ‰«æ checkpoints æ ¹ç›®å½•ï¼Œå¯»æ‰¾æœ€è¿‘ä¸€æ¬¡è¿è¡Œçš„ last.ckpt
    checkpoints_root = os.path.join(project_root, "checkpoints")
    latest_ckpt = get_latest_checkpoint(checkpoints_root)

    print("-" * 50)
    print(f"ðŸ•’ æœ¬æ¬¡è¿è¡Œ ID: {run_id}")
    print(f"ðŸ“‚ æ¨¡åž‹ä¿å­˜è·¯å¾„: {current_ckpt_dir}")
    print(f"ðŸ“ æ—¥å¿—ä¿å­˜è·¯å¾„: {current_log_dir}")

    if latest_ckpt:
        print(f"â™»ï¸ å‘çŽ°åŽ†å²å­˜æ¡£: {latest_ckpt}")
        print(f"â™»ï¸ ç³»ç»Ÿå°†åŠ è½½è¯¥æƒé‡ï¼Œå¹¶ç»§ç»­è®­ç»ƒ (Logså°†å†™å…¥æ–°çš„æ—¶é—´æ–‡ä»¶å¤¹)")
    else:
        print("âœ¨ æœªå‘çŽ°åŽ†å²å­˜æ¡£ï¼Œå¼€å§‹å…¨æ–°è®­ç»ƒ...")
    print("-" * 50)

    # å¼€å§‹è®­ç»ƒ
    trainer.fit(model, train_loader, val_loader, ckpt_path=latest_ckpt)


if __name__ == "__main__":
    main()